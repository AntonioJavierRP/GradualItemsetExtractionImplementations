{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación completa en Apache Spark del algortimo propuesto usando PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import itertools\n",
    "#import os\n",
    "#import shutil\n",
    "#import sys\n",
    "#import argparse\n",
    "\n",
    "from scipy.sparse import csr_matrix as csr\n",
    "#from scipy.sparse import random\n",
    "\n",
    "# Dependiendo de como se configure Spark las siguientes líneas pueden ser necesarias.\n",
    "#import findspark\n",
    "#findspark.init('/home/antoniojavier/spark-2.4.4-bin-hadoop2.7')\n",
    "import pyspark as spark\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones necesarias para el cálculo del coeficiente de correlación de rango difuso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-norma de Lukasiewicz.\n",
    "def T_norm(x,y):\n",
    "    return max(0,x+y-1)\n",
    "\n",
    "# TL-E-Ordenacion fuertemente completa en R.  \n",
    "def Fuzzy_ordering(x,y,r):\n",
    "    return min(1,max(0,1-((x-y)/r)))\n",
    "\n",
    "# Relación difusa estricta (Rx o Ry)\n",
    "def Fuzzy_relation(x1,x2,r):\n",
    "    return 1-Fuzzy_ordering(x2,x1,r)\n",
    "\n",
    "# El grado de concordancia entre dos pares dado un r se calculará entonces de la siguiente forma.\n",
    "def Concordance_degree(pair1,pair2,r):\n",
    "    return T_norm(Fuzzy_relation(pair1[0], pair1[1],r), Fuzzy_relation(pair2[0], pair2[1],r))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de la matriz de grados de concordancia\n",
    "Esta función devuelve la matriz de grados de concordancia del itemset de tamaño k=2 que se le pase como argumento, junto con un r y el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    Estas otras combinaciones nos las evaluaremos ya que solo habrá itemsets de tamaño 2 del \\n    tipo (('atributo1', '>'), ('atributo2', '>')) y (('atributo1', '>'), ('atributo2', '<'))\\n\\n    if op1 == '<' and op2 == '>': \\n        for i in range(N):\\n            for j in range(N):\\n                if i != j:\\n                    matrix[i,j] = Concordance_degree([at1[j], at1[i]],[at2[i], at2[j]],r)          \\n    \\n    \\n\\n    if op1 == '<' and op2 == '<': \\n        for i in range(N):\\n            for j in range(N):\\n                if i != j:\\n                    matrix[i,j] = Concordance_degree([at1[j], at1[i]],[at2[j], at2[i]],r)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utiliza float de 16 bits en lugar del float estándar en python que utiliza 64 bits.\n",
    "def generate_matrix(candidate, df, r):\n",
    "    \n",
    "    \n",
    "    at1 = pd.to_numeric(df.value[candidate[0][0]])\n",
    "    op1 = candidate[0][1]\n",
    "    \n",
    "    at2 = pd.to_numeric(df.value[candidate[1][0]])\n",
    "    op2 = candidate[1][1]\n",
    "    \n",
    "    N = len(at1)     # N es el número de transacciones\n",
    "    matrix = np.zeros((N, N), dtype = np.float16)\n",
    "    \n",
    "    if op1 == '>' and op2 == '>': \n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if i != j:\n",
    "                    matrix[i,j] = Concordance_degree([at1[i], at1[j]],[at2[i], at2[j]],r)\n",
    "                    \n",
    "    if op1 == '>' and op2 == '<': \n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if i != j:\n",
    "                    matrix[i,j] = Concordance_degree([at1[i], at1[j]],[at2[j], at2[i]],r)\n",
    "                    \n",
    "    return (tuple(candidate), csr(matrix))\n",
    "                \n",
    "\"\"\"\n",
    "    Estas otras combinaciones nos las evaluaremos ya que solo habrá itemsets de tamaño 2 del \n",
    "    tipo (('atributo1', '>'), ('atributo2', '>')) y (('atributo1', '>'), ('atributo2', '<'))\n",
    "\n",
    "    if op1 == '<' and op2 == '>': \n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if i != j:\n",
    "                    matrix[i,j] = Concordance_degree([at1[j], at1[i]],[at2[i], at2[j]],r)          \n",
    "    \n",
    "    \n",
    "\n",
    "    if op1 == '<' and op2 == '<': \n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if i != j:\n",
    "                    matrix[i,j] = Concordance_degree([at1[j], at1[i]],[at2[j], at2[i]],r)\n",
    "\"\"\"                 \n",
    "    \n",
    "\n",
    "\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de la matriz de grados de concordancia. 8 bits\n",
    "Variante de la función anterior para el caso en el que se elija una precisión de 8bits, se utilizan enteros de 8 bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliza enteros sin signo de 8 bits en lugar del float estándar en python que utiliza 64 bits.\n",
    "def generate_matrix_int(candidate, df, r):\n",
    "    \n",
    "    \n",
    "    at1 = pd.to_numeric(df.value[candidate[0][0]])\n",
    "    op1 = candidate[0][1]\n",
    "    \n",
    "    at2 = pd.to_numeric(df.value[candidate[1][0]])\n",
    "    op2 = candidate[1][1]\n",
    "    \n",
    "    N = len(at1)     # N es el número de transacciones\n",
    "    matrix = np.zeros((N, N), dtype = np.uint)\n",
    "    \n",
    "    if op1 == '>' and op2 == '>': \n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if i != j:\n",
    "                    matrix[i,j] = (255*Concordance_degree([at1[i], at1[j]],[at2[i], at2[j]],r)) \n",
    "                    \n",
    "    if op1 == '>' and op2 == '<': \n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if i != j:\n",
    "                    matrix[i,j] = (255*Concordance_degree([at1[i], at1[j]],[at2[j], at2[i]],r))\n",
    "                    \n",
    "    return (tuple(candidate), csr(matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo del soporte\n",
    "Devuelve el soporte de la matriz que se le pasa como argumento como la suma de todos sus elementos dividido entre N(N-1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devuelve el soporte de la matriz\n",
    "def compute_support(m):\n",
    "    N = m.shape[1]\n",
    "    return csr.sum(m)/(N*((N-1)/2))   # También es posible usar np.sum directamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo del soporte para 8 bits.\n",
    "Variante de la función anterior para el caso en el que se elija una precisión de 8bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devuelve el soporte de la matriz\n",
    "def compute_support_int(m):\n",
    "    N = m.shape[1]\n",
    "    return csr.sum(m/255)/(N*((N-1)/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validación del candidato combinado\n",
    "\n",
    "Función que devuelve la combinación de los itemsets pasados como argumento si esta combinación es viable teniendo en cuenta los itemsets frecuentes de la iteración anterior, evaluando si:\n",
    "\n",
    "* Tienen al menos k-2 elementos en común\n",
    "* Todos sus subconjuntos de itemset graduales de tamaño k-1 fueron frecuentes en la iteración o fase anterior.\n",
    "\n",
    "La condición de si ya han sido generados se evalúa después de esta función con un distinct() de la lista obtenida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinedCandidates(itemset_pair, previous_frequent_itemsets):\n",
    "    #previous_frequent_itemsets = [set(i) for i in previous_frequent_itemsetsk]\n",
    "    result = ()\n",
    "    difference = set(itemset_pair[1])-set(itemset_pair[0])\n",
    "    if(len(difference) == 1):\n",
    "        result =  itemset_pair[0] + tuple(difference)\n",
    "        combinations = [x for x in itertools.combinations(result, len(result)-1)]\n",
    "        if not all(i in previous_frequent_itemsets for i in combinations):\n",
    "            result = ()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinación de matrices\n",
    "Función que recibe como argumento a un itemset combinado y la lista de itemsets frecuentes de la iteración anterior, con sus respectivas matrices. Devuelve el itemset y la matriz resultante de la combinación de las matrices de dos de los itemsets de tamaño k-1 que lo conforman, quedándonos con el mínimo de cada uno de sus elementos (T-norma de Gödel de las matrices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeCombinedMatrix(itemset, previous_frequent):\n",
    "    \n",
    "    itemset1 = itemset[:-1]\n",
    "    itemset2 = itemset[1:]\n",
    "    #itemset2 = itemset[:-2]+itemset[-1]\n",
    "    \n",
    "    \n",
    "    matriz_resultante = csr.minimum(previous_frequent.value[itemset1], previous_frequent.value[itemset2])\n",
    "    itemset_resultante = itemset1 + tuple(set(itemset2)-set(itemset1))\n",
    "    \n",
    "    return (itemset_resultante, matriz_resultante)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función principal del algoritmo.\n",
    "Devuelve la lista de todos los itemsets frecuentes en el conjunto de datos dado, que superen un cierto soporte mínimo. Recibe como entrada (argumentos) el SparkContext, el conjunto de datos(distribuido en el cluster), el soporte mínimo, un r para realizar las llamadas al cálculo de grado de correlación entre atributos. (Usa una precisión de 16 bits para almacenar los grados de concordancia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFrequentItemsets(sc, distDataset, min_supp, r):\n",
    "    \n",
    "    # Primera fase del algoritmo\n",
    "    candidates = []\n",
    "\n",
    "    n_att = len(distDataset.value.columns)\n",
    "\n",
    "    # Añadimos a candidates todas los posibles itemsets de tamaño 2 (exceptuando equivalentes)\n",
    "    for i in range(n_att):\n",
    "        for j in range(i+1,n_att):\n",
    "            candidates += [[(distDataset.value.columns[i],'>'),(distDataset.value.columns[j],'>')]]\n",
    "            candidates += [[(distDataset.value.columns[i],'>'),(distDataset.value.columns[j],'<')]]\n",
    "\n",
    "    \n",
    "    # Evaluamos cada uno de los posibles candidatos de forma distribuida en el cluster, quedándonos únicamente\n",
    "    # con los itemsets frecuentes de tamaño 2 junto con su matriz asociada.\n",
    "    frequent_itemsets_k = sc.parallelize(candidates).map(lambda x: generate_matrix(x, distDataset, r))\\\n",
    "                                                    .filter(lambda x: compute_support(x[1]) >= min_supp)\\\n",
    "                                                    .collect()\n",
    "    \n",
    "\n",
    "    # Segunda fase del algoritmo\n",
    "\n",
    "    frequent_itemsets_list = []\n",
    "    # Repetimos este proceso hasta que la lista de itemsets frecuentes generados en la anterior iteración esté vacía\n",
    "    while(frequent_itemsets_k):\n",
    "        # Creamos una copia de los itemsets frecuentes de la fase o iteración anterior y sus respectivas matrices\n",
    "        # en cada uno de los nodos del cluster.\n",
    "        previous_frequent_dict = sc.broadcast(dict(frequent_itemsets_k))\n",
    "        previous_frequent_itemsets = [item for item in previous_frequent_dict.value] # Lista con los itemsets frecuentes anterior\n",
    "        frequent_itemsets_list += previous_frequent_itemsets\n",
    "        candidates_combinations = [x for x in itertools.combinations(previous_frequent_itemsets, 2)]\n",
    "\n",
    "        # Evaluamos las posibles combinaciones de candidatos de forma distribuida en el cluster, quedándonos únicamente\n",
    "        # con los viables cuya matriz supere el soporte mínimo.\n",
    "        frequent_itemsets_k = sc.parallelize(candidates_combinations)\\\n",
    "                             .map(lambda x: combinedCandidates(x,previous_frequent_itemsets))\\\n",
    "                             .filter(lambda x: x)\\\n",
    "                             .distinct()\\\n",
    "                             .map(lambda x: ComputeCombinedMatrix(x, previous_frequent_dict))\\\n",
    "                             .filter(lambda x: compute_support(x[1]) >= min_supp)\\\n",
    "                             .collect()\n",
    "\n",
    "    \n",
    "    # Eliminamos el contexto actual.\n",
    "    sc.stop()\n",
    "    return frequent_itemsets_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función principal igual que la anterior pero para una precisión de 8 bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFrequentItemsets_int(sc, distDataset, min_supp, r):\n",
    "    \n",
    "    # Primera fase del algoritmo\n",
    "    candidates = []\n",
    "\n",
    "    n_att = len(distDataset.value.columns)\n",
    "\n",
    "    # Añadimos a candidates todas los posibles itemsets de tamaño 2 (exceptuando equivalentes)\n",
    "    for i in range(n_att):\n",
    "        for j in range(i+1,n_att):\n",
    "            candidates += [[(distDataset.value.columns[i],'>'),(distDataset.value.columns[j],'>')]]\n",
    "            candidates += [[(distDataset.value.columns[i],'>'),(distDataset.value.columns[j],'<')]]\n",
    "\n",
    "    # Evaluamos cada uno de los posibles candidatos de forma distribuida en el cluster, quedándonos únicamente\n",
    "    # con los itemsets frecuentes de tamaño 2 junto con su matriz asociada.\n",
    "    frequent_itemsets_k = sc.parallelize(candidates).map(lambda x: generate_matrix_int(x, distDataset, r))\\\n",
    "                                                    .filter(lambda x: compute_support_int(x[1]) >= min_supp)\\\n",
    "                                                    .collect()\n",
    "\n",
    "    \n",
    "\n",
    "    # Segunda fase del algoritmo\n",
    "\n",
    "    frequent_itemsets_list = []\n",
    "    # Repetimos este proceso hasta que la lista de itemsets frecuentes generados en la anterior iteración esté vacía\n",
    "    while(frequent_itemsets_k):\n",
    "        # Creamos una copia de los itemsets frecuentes de la fase o iteración anterior y sus respectivas matrices\n",
    "        # en cada uno de los nodos del cluster.\n",
    "        previous_frequent_dict = sc.broadcast(dict(frequent_itemsets_k))\n",
    "        previous_frequent_itemsets = [item for item in previous_frequent_dict.value] # Lista con los itemsets frecuentes anterior\n",
    "        frequent_itemsets_list += previous_frequent_itemsets\n",
    "        candidates_combinations = [x for x in itertools.combinations(previous_frequent_itemsets, 2)]\n",
    "\n",
    "        # Evaluamos las posibles combinaciones de candidatos de forma distribuida en el cluster, quedándonos únicamente\n",
    "        # con los viables cuya matriz supere el soporte mínimo.\n",
    "        frequent_itemsets_k = sc.parallelize(candidates_combinations)\\\n",
    "                             .map(lambda x: combinedCandidates(x,previous_frequent_itemsets))\\\n",
    "                             .filter(lambda x: x)\\\n",
    "                             .distinct()\\\n",
    "                             .map(lambda x: ComputeCombinedMatrix(x, previous_frequent_dict))\\\n",
    "                             .filter(lambda x: compute_support_int(x[1]) >= min_supp)\\\n",
    "                             .collect()\n",
    "\n",
    "    # Eliminamos el contexto actual.\n",
    "    sc.stop()\n",
    "    return frequent_itemsets_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializamos el SparkContext especificando el nombre de la aplicación y el número de núcleos (en caso de realizar una ejecución en máquina local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializacion del contexto\n",
    "def createSparkContext():\n",
    "    sc_conf = spark.SparkConf()\n",
    "    sc_conf.setMaster(\"local[*]\")    # Aqui podemos especificar el número de núcleos del procesador que queremos que se utilicen en local\n",
    "    sc_conf.setAppName(\"pattern mining\")\n",
    "    #sc_conf.set('spark.executor.instances', '2')\n",
    "    #sc_conf.set('spark.executor.memory', '2g')\n",
    "    #sc_conf.set('spark.executor.cores', '1')\n",
    "    #sc_conf.set('spark.cores.max', '1')\n",
    "    #sc_conf.set('spark.logConf', True)\n",
    "\n",
    "    sc = spark.SparkContext(conf=sc_conf)\n",
    "\n",
    "    return sc \n",
    "\n",
    "sc = createSparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución directa, asignando aqui valores que queremos a las variables\n",
    "En ImplementacionSpark.py podemos realizar la ejecución y asignación de parámetros desde la línea de comandos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución del algoritmo:  14.219066858291626\n",
      "\n",
      "\n",
      "Itemsets frecuentes obtenidos:\n",
      "\n",
      "\n",
      "(('fixed acidity', '>'), ('volatile acidity', '<')) \n",
      "\n",
      "(('fixed acidity', '>'), ('citric acid', '>')) \n",
      "\n",
      "(('fixed acidity', '>'), ('free sulfur dioxide', '>')) \n",
      "\n",
      "(('fixed acidity', '>'), ('total sulfur dioxide', '>')) \n",
      "\n",
      "(('fixed acidity', '>'), ('density', '>')) \n",
      "\n",
      "(('fixed acidity', '>'), ('pH', '<')) \n",
      "\n",
      "(('fixed acidity', '>'), ('sulphates', '>')) \n",
      "\n",
      "(('fixed acidity', '>'), ('alcohol', '<')) \n",
      "\n",
      "(('volatile acidity', '>'), ('citric acid', '<')) \n",
      "\n",
      "(('volatile acidity', '>'), ('free sulfur dioxide', '>')) \n",
      "\n",
      "(('volatile acidity', '>'), ('free sulfur dioxide', '<')) \n",
      "\n",
      "(('volatile acidity', '>'), ('total sulfur dioxide', '>')) \n",
      "\n",
      "(('volatile acidity', '>'), ('total sulfur dioxide', '<')) \n",
      "\n",
      "(('volatile acidity', '>'), ('density', '>')) \n",
      "\n",
      "(('volatile acidity', '>'), ('density', '<')) \n",
      "\n",
      "(('volatile acidity', '>'), ('pH', '>')) \n",
      "\n",
      "(('volatile acidity', '>'), ('sulphates', '<')) \n",
      "\n",
      "(('volatile acidity', '>'), ('alcohol', '>')) \n",
      "\n",
      "(('citric acid', '>'), ('chlorides', '>')) \n",
      "\n",
      "(('citric acid', '>'), ('free sulfur dioxide', '>')) \n",
      "\n",
      "(('citric acid', '>'), ('total sulfur dioxide', '>')) \n",
      "\n",
      "(('citric acid', '>'), ('density', '>')) \n",
      "\n",
      "(('citric acid', '>'), ('density', '<')) \n",
      "\n",
      "(('citric acid', '>'), ('pH', '<')) \n",
      "\n",
      "(('citric acid', '>'), ('sulphates', '>')) \n",
      "\n",
      "(('citric acid', '>'), ('alcohol', '>')) \n",
      "\n",
      "(('citric acid', '>'), ('alcohol', '<')) \n",
      "\n",
      "(('residual sugar', '>'), ('free sulfur dioxide', '>')) \n",
      "\n",
      "(('residual sugar', '>'), ('total sulfur dioxide', '>')) \n",
      "\n",
      "(('residual sugar', '>'), ('density', '>')) \n",
      "\n",
      "(('residual sugar', '>'), ('alcohol', '>')) \n",
      "\n",
      "(('chlorides', '>'), ('free sulfur dioxide', '>')) \n",
      "\n",
      "(('chlorides', '>'), ('pH', '<')) \n",
      "\n",
      "(('chlorides', '>'), ('sulphates', '>')) \n",
      "\n",
      "(('free sulfur dioxide', '>'), ('total sulfur dioxide', '>')) \n",
      "\n",
      "(('free sulfur dioxide', '>'), ('density', '>')) \n",
      "\n",
      "(('free sulfur dioxide', '>'), ('pH', '<')) \n",
      "\n",
      "(('free sulfur dioxide', '>'), ('sulphates', '>')) \n",
      "\n",
      "(('total sulfur dioxide', '>'), ('density', '>')) \n",
      "\n",
      "(('total sulfur dioxide', '>'), ('pH', '<')) \n",
      "\n",
      "(('total sulfur dioxide', '>'), ('sulphates', '>')) \n",
      "\n",
      "(('total sulfur dioxide', '>'), ('alcohol', '>')) \n",
      "\n",
      "(('total sulfur dioxide', '>'), ('alcohol', '<')) \n",
      "\n",
      "(('density', '>'), ('pH', '>')) \n",
      "\n",
      "(('density', '>'), ('pH', '<')) \n",
      "\n",
      "(('density', '>'), ('sulphates', '>')) \n",
      "\n",
      "(('density', '>'), ('alcohol', '<')) \n",
      "\n",
      "(('pH', '>'), ('sulphates', '<')) \n",
      "\n",
      "(('pH', '>'), ('alcohol', '>')) \n",
      "\n",
      "(('sulphates', '>'), ('alcohol', '>')) \n",
      "\n",
      "(('fixed acidity', '>'), ('free sulfur dioxide', '>'), ('total sulfur dioxide', '>')) \n",
      "\n",
      "(('citric acid', '>'), ('total sulfur dioxide', '>'), ('pH', '<')) \n",
      "\n",
      "(('citric acid', '>'), ('total sulfur dioxide', '>'), ('sulphates', '>')) \n",
      "\n",
      "(('fixed acidity', '>'), ('citric acid', '>'), ('pH', '<')) \n",
      "\n",
      "(('free sulfur dioxide', '>'), ('total sulfur dioxide', '>'), ('pH', '<')) \n",
      "\n",
      "(('free sulfur dioxide', '>'), ('total sulfur dioxide', '>'), ('sulphates', '>')) \n",
      "\n",
      "(('fixed acidity', '>'), ('density', '>'), ('pH', '<')) \n",
      "\n",
      "(('citric acid', '>'), ('free sulfur dioxide', '>'), ('total sulfur dioxide', '>')) \n",
      "\n",
      "(('free sulfur dioxide', '>'), ('total sulfur dioxide', '>'), ('density', '>')) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejecución\n",
    "r = 0.098\n",
    "\n",
    "min_supp = 0.3\n",
    "\n",
    "dataset_file = \"winequality-red.csv\"\n",
    "\n",
    "precision = '16bits'\n",
    "\n",
    "n_att = 12\n",
    "\n",
    "n_trans = 100\n",
    "\n",
    "sep = ';'\n",
    "\n",
    "if(dataset_file == 'pd_speech_features.csv'):\n",
    "    df = pd.read_csv(\"pd_speech_features.csv\",sep = \",\",skiprows = 1)   # Lo ponemos aparte porque tiene una fila extra y dos columnas que debemos quitar\n",
    "    my_data = df.iloc[:n_trans,2:n_att+2]\n",
    "else:\n",
    "    # Cargomos los datos en un dataframe de pandas.\n",
    "    #df = pd.read_csv(\"seizure.csv\",sep = \",\")\n",
    "    df = pd.read_csv(dataset_file,sep = sep)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Nos quedamos con un subconjunto total de las transacciones y de los atributos para adaptarlo a la capacidad de nuestro hardware\n",
    "#my_data = df.iloc[:80,1:-149]\n",
    "my_data = df.iloc[:50,:]\n",
    "#my_data = df.iloc[:100,1:-743]\n",
    "\n",
    "\n",
    "# Normalizo los datos (usando min-max scaling)\n",
    "normalized_data = (my_data - my_data.min())/(my_data.max() - my_data.min())\n",
    "\n",
    "datadist = sc.broadcast(normalized_data)\n",
    "\n",
    "if precision == '8bits':\n",
    "        t1 = time.time()\n",
    "    \n",
    "        result = extractFrequentItemsets_int(sc,datadist, min_supp,r)\n",
    "    \n",
    "        t2 = time.time()\n",
    "else:  \n",
    "        t1 = time.time()\n",
    "        \n",
    "        result = extractFrequentItemsets(sc,datadist, min_supp,r)\n",
    "        \n",
    "        t2 = time.time()\n",
    "\n",
    "print(\"Tiempo de ejecución del algoritmo: \", t2-t1)\n",
    "print(\"\\n\\nItemsets frecuentes obtenidos:\\n\\n\")\n",
    "for i in result:\n",
    "        print(i,\"\\n\")\n",
    "# Es necesario reinicializar el SparkContext si queremos ejecutar esta celda mas de una vez."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
